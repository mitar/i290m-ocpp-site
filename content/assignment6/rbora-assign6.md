Title: Exploring Common Crawl
Date: 2013-12-16 10:20
Category: Open Collaboration and Peer Production
Tags: pelican, publishing
Slug: commoncrawl
Author: Renu Bora
Summary: Assignment 6

ASSIGNMENT 6: Where does the funding for you community come from? Is there corporate sponsorship? A foundation that backs it? Do users donate? How does this affect the community's cooperative dynamics? Are there competing projects? How would you describe your project's role in the greater technical ecosystem?

Gil Elbaz, a successful entrepreneur, seems to be the sole founder and of the Common Crawl Foundation.  They have donation links and corporate sponsorship opportunities, but they don't seem to have any official corporate sponsors yet (though several leaders of companies contribute non-financially). It is not clear how many donations they receive from their donation links, or other sources. Also, Gil Elbaz's company Factual has a few volunteers with Common Crawl, which is not surprising given the personal connections as well as the "big internet data" overlap in the Factual products vs. Common Crawl product. There seems to be no competing product. However, there is a growing competing service, which is not well-defined, which is that for some people (perhaps an increasing amount), it makes more sense to crawl the web oneself to obtain a specific and more manageable corpus.  For specific projects or data needs, such as understanding the networks of one specific group's website, their Google group mentioned using either Nutch directly (which Common Crawl itself uses) or one of the graphs of webdatacommons.org (which used Common Crawl itself. However, one person mentioned that using Nutch by oneself might easily lead to spam (2-3 hops of crawling typically leads to spam, porn, and other "dirty data."). Common Crawl could in the future highlight its "clean data" as one of its more valuable benefits. At the end of 2012, the search engine blekko.com began an ongoing donation of its own search ranking metadata of 140 million websites and 22 billion webpages to Common Crawl. This data helps Common Crawl minimize spam, porn, and other questionable websites. This ecosystem development creates a dependency on a private company, but the benefit seems well worth it, and blekko.com states that it believes in an open and transparent web.

Over time, people will continue to evolve practices around when it's better to perform one's own crawl vs. when to use Common Crawl's. Common Crawl does seem to be an important resource for public policy and possibly even long-term innovation, in that researchers who need to work with huge datasets increasingly find companies like Google, Amazon, and Facebook appealing, and there is therefore a "brain drain" from academics to private industry. The public generally has little funds or resources to maintain huge datasets compared with these types of company. Technical/data innovation both for researchers and for technology companies (from startup to corporations) may depend on having such resources in the general ecosystem. I don't have space for a full argument or position here, since the domain is a huge, complex policy and business problem space, but it resembles many arguments favoring distributed, collaborative, public good and synergy vs. private industrial silos.

 
